# 一、机器学习系统的设计
在构建一个机器学习系统时，我们通常会先思考一些方法。

我们以一个垃圾邮件分类算法为例。

spam（垃圾邮件）分类算法是监督学习的算法，我们选取一个由100个代表spam/not spam单词组成的特征向量x。
> 在实践中，通常采用10000到50000个单词作为特征向量。这里只是简单的说明了垃圾邮件分类器的原理。

如果在一封邮件中，出现了我们选取为特征的单词，则对应的xj=1，否则xj=0。

为了构建这个垃圾邮件分类算法，我们通常采用的方法是：
> 1. 收集大量垃圾邮件和非垃圾邮件的案例 。
> 2. 基于邮件的路由信息开发一系列复杂的特征。
> 3. 基于邮件正文信息开发一系列复杂特征，比喻说“截词”处理。
> 4. 为刻意写错的单词构造复杂的算法。

在构建一个机器学习算法时，选择一个系统的方法，而不是突发奇想地突然开始，能有效地提高算法地效率。

下文误差分析会告诉我们如何使用一个更系统地方式，选择一个比较合适的方法。

## 1.误差分析（error analysis)
在搭建机器学习系统时，我们在较短的时间内，比如说24小时内先搭建一个简单的系统。

即便是搭建的系统性能很差，但只要我们能很快的得到结果，最后用过cross_validation集检验数据。通过刻画学习曲线以及检验误差，来找出算法的high bias/high variance问题，再来决定是使用增加数据集或者加入特征变量或者其他方法。

采用这种方法，是为了让证据领导我们优化，而不是直觉领导优化。

除了刻画学习曲线之外，一件非常有用地事是误差分析。当我们构造垃圾邮件分类器时，我们看一下cross_validation集哪些邮件被算法错误地分类，从被错误分类地算法中找出系统性地规律：什么类型地邮件容易被错误分类。

经常这样做，能启发我们构造新的特征变量；或者告诉我们系统地短处，然后启发我们如何去进行优化。

构建一个机器学习算法推荐地方法：
> 1. 简单构建一个能快速实现地算法，让后用cross_validation集进行验证。
> 2. 刻画学习曲线，决定是增加更多地数据，还是添加新的特征，或者其他选择。
> 3. 进行**误差分析**：人工检查cross_validation集中被错误分类的邮件，看看这些实例是否有系统化的趋势。

以我们的垃圾邮件过滤器为例，误差分析要做的是检验交叉验证集中我们的算法产生错误预测的所有邮件，看：是否能将这些邮件按照类分组。例如医药品垃圾邮件，仿冒品垃圾邮件或者密码窃取邮件等。然后看分类器对哪一组邮件的预测误差最大，并着手优化。

**敲黑板**：误差分析并不总能帮助我们判断应该采取怎样的行动，有时我们需要更换模型。

在模型比较中，我们通常用cross_validaton集的误差来判断那个模型会更好。

举个栗子：
> 在垃圾邮件分类器中，对于“我们是否应该将discount/discounts/discounting处理为同一个词？”，如果这样做可以改善我们的算法，我们会采用一些截词软件。<br>
> <br>
> 误差分析不能帮助我们做出判断，我们只能采用和尝试采用截词软件和不采用截词软件这两种方案，然后根据数值检验结果来判断哪一种更好。

在构造学习算法的时候，我们会尝试不同的想法——词干提取/区分大小写等等。我们通过构建一个量化的数值评估，直观的看到误差变大了还是变小了。

量化的比较本质上就是通过比较不同模型的错误率进行error analysis，选出最佳模型。

## 2.类偏斜的误差度量
前面我们提到了误差分析，以及设置误差度量值的重要性。通过设定某个实数来评估学习算法，并衡量它的表现。

有一个非常重要的事情就是选择合适的误差度量值，这有时会对学习算法造成非常微妙的影响，这件重要的事情就是类偏斜（skewed classes）。

类偏斜情况表现为我们的训练集中有非常多的同一种类实例，只有很少或没有其他类实例。

例如我们希望用算法来预测肿瘤是否是恶性的，在我们的训练集中，只有0.5%的实例是恶性肿瘤。假设我们编写一个非学习而来的算法，在所有情况下都预测肿瘤是良性的，那么误差只有 0.5%。然而我们通过训练而得到的神经网络算法却有1%的误差。这时，误差的大小是不能视为评判算法效果的依据的。

**查准率（Precision）**和**查全率（Recall)**是新的度量标准。

引入定义Precision和Recall的变量：
1. True Positive(TP)：预测为真，实际为真。
2. False Positive(FP)：预测为真，实际为假。
3. True Negative(TN)：预测为假，实际为假。
4. False Negative(FN)：预测为假，实际为真。

* Precision = TP / (TP+FP)：对所有预测是恶性肿瘤的病人，实际上是恶性肿瘤的病人的比例。越高月好。

* Recall = TP / (TP+FP)：在所有恶性肿瘤病人中，成功预测由恶性肿瘤的病人的比例。越高越好。

上边我们提到的所有预测为良性的算法，其Recall=0。

![ML-seventh1](https://github.com/yiyading/NLP-and-ML/blob/master/img_ML2/ML-seventh1.png)

## 3.查准率和查全率之间的权衡
在上面我们提到Precision和Recall作为类偏斜问题的评估度量值。这两个概念的度量方向不同，但在实际问题中，我们通常希望Recaall和Precision相对平衡。

我们延用上一节中恶性肿瘤的例子。

假设我们算法的结果在0-1之间，我们设置阈值为0.5来判断真假。

![ML-seventh2](https://github.com/yiyading/NLP-and-ML/blob/master/img_ML2/ML-seventh2.png)

如果我们只希望在非常确信的情况下预测为真（恶性肿瘤），那么我们需要非常高的Precision，我们提高阈值，可以将其设置为0.7、0.9。

如果我们提高Recall是所有有可能是恶性肿瘤的得到进一步检查，可以将阈值设置为0.3、0.1。虽然能提高Recall，但是这会降低Precision。

不同阈值情况下，Recall和Precission的关系如下图所示：

![ML-seventh3](https://github.com/yiyading/NLP-and-ML/blob/master/img_ML2/ML-seventh3.png)

> **我们希望一种方法能帮助我们选择这个阈值（threshold）**。一种方法是计算F1值（F1 Score），其计算公式为：
> > ![ML-seventh4](https://github.com/yiyading/NLP-and-ML/blob/master/img_ML2/ML-seventh4.png)
> 我们选择使F1值最高的阈值

## 4.数据的重要性

在机器学习中，对于一个性能不那么好的算法，如果由足够的数据对其进行训练，其预测效果要优于没有足够数据但性能好的算法。

下图展示了四个不同的算法在不同数据量下的准确率。

![ML-seventh5](https://github.com/yiyading/NLP-and-ML/blob/master/img_ML2/ML-seventh5.png)

# 二、支持向量机（SVM）
在机器学习中，不同算法的性能不同。

但更加重要的是，所创建的大量数据在应用这些算法时，表现情况通常依赖于工程师的水平。比如，你为学习算法所设计的特征量的选择，以及如何正则化参数等等。

在工业界和学术界有一种更加强大的算法，即支持向量机（support vector machines），或简称SVM。

SVM在学习复杂的非线性方程式提供了一种更加清晰，更加强大的方式。

## 1.优化目标

![ML-seventh6](https://github.com/yiyading/NLP-and-ML/blob/master/img_ML2/ML-seventh6.png)

如上图所示。在逻辑回归中我们已经非常熟悉这种假设函数的形式和右边的S型激活函数，我们使用z来代替θ.T\*x。

我们思考一下逻辑回归做了什么？

如果有一个y=1的样本，我们希望h趋近于1。因此我们如果想正确的分类此样本，需要z远大于0，此时在上图中不难发现逻辑回归的输出将趋近于1；同理，当z远小于0时，h趋近于0，即逻辑回归的输出趋近于0。

![ML-seventh7](https://github.com/yiyading/NLP-and-ML/blob/master/img_ML2/ML-seventh7.png)

上图逻辑回归中的代价函数，我们先忽略1/m这一项。从这个代价函数中，我们能看到每一个样本(x,y)都会对总代价函数做出贡献。

当y=1时，上边的代价函数会消去(1-y)()那一项。上图中左下的那个图表示此时的代价函数随z变化的增减。当z增大时，也就是θ.T*x增大时，对代价函数的影响也会非常小。

为什么逻辑回归观察到正样本y=1时，试图将θ.T*x设置的非常大？因为，在代价函数中这一项会变得非常小。

**通过对逻辑回归的修改构建SVM**

**1. 构造新的代价函数cost1(z)和cost0(z)**
在下图中，我们对逻辑回归中的代价函数分成了两种情况来讨论。

![ML-seventh8](https://github.com/yiyading/NLP-and-ML/blob/master/img_ML2/ML-seventh8.png)

上图是代价函数中y=1的情况，此时的-log函数随着z增大而减小，我们以1为界限将其转化为一条水平线和一条直线，构造成新的代价函数cost1(z)。

同理下图为代价函数中y=0的情况。构造新的代价函数cost0(z).

![ML-seventh9](https://github.com/yiyading/NLP-and-ML/blob/master/img_ML2/ML-seventh9.png)

上面这两个代价函数中下标1和0分别表示y=1和y=0的情况。

![ML-seventh10](https://github.com/yiyading/NLP-and-ML/blob/master/img_ML2/ML-seventh10.png)

我们在逻辑回归中使用代价函数J(θ)，上图将代价函数的负号被放到了方程内部。对于向量机而言，实质上我们要将这些log函数替换为cost1(z)和cost0(z)，也就是cost1(θ.T*x)和cost0(θ.T*x)。

**2. 正则化参数**

按照SVM的惯例，我们去除逻辑回归中1/m这一项，之所以去除1/m这一项，仅仅是因为人们使用SVM中的习惯。因为1/m仅仅是一个常量，在优化的过程中，无论是否有这一项，都能得到最优的θ。

在逻辑回归中，我们使用λ来权衡。逻辑回归的代价函数本质上A+λ*B来达到优化A的目的。（B为正则化，A为代价计算）

在SVM中我们使用一个不同的参数C代替λ来权衡这两项，即C*A+B。在逻辑回归中，如果λ很大，意味着给B更大的权重；同理，在SVM中如果C很小，相应的B的权重会比A的大。

因此就得到了支持向量机（SVM）中整个优化目标函数。如下图所示：

![ML-seventh11](https://github.com/yiyading/NLP-and-ML/blob/master/img_ML2/ML-seventh11.png)

**有别于逻辑回归输出的是概率。在SVM中，我们最小化后的代价函数h获得的参数θ，SVM来所作的是它来直接预测y=1或者y=0**。因此当θ.T\*x大于等于0时，会直接预测y=1。这就是SVM数学上的意义。

## 2.大边界的直接理解
人们有时将SVM看作大间距分类器。即**用最大间距将样本分离开来**。
> 这一节只从图像上说明SVM被看作大间距分类器的原因。下一节会从数学原理上分析原因。

![ML-seventh12](https://github.com/yiyading/NLP-and-ML/blob/master/img_ML2/ML-seventh12.png)

上图是SVM模型的代价函数，左边cost1(z)用于正样本，右边cost2(z)用于负样本。

我们观察一下cost1(z)，对于一个正样本y=1时，只有当z大于等于1时，也就是θ.T\*x大于等于1时，从上图可看出cost1(z)=0。事实上当z大于等于0时，我们就能将正样本恰当分离出来。

同理，对于负样本y=0时，当z小于等于0时就能将负样本恰当的分离出来。

**但SVM的要求是，正样本z大于等于1，负样本小于等于-1，才能将样本分离出来。也就是向SVM中嵌入了一个额外的安全因子（或者说安全间距因子）**。

在SVM中这个因子的作用是什么？接下来我们会考虑一个特例。

我们将常数C设置成一个非常大的值，比如10000或者更大。如果C非常大，则最小化优化函数时，我们希望得到第一项为0的最优解。

下图是SVM的优化函数：

![ML-seventh13](https://github.com/yiyading/NLP-and-ML/blob/master/img_ML2/ML-seventh13.png)

当我们输入一个训练样本标签y=1，你想令第一项为0，则需要找到一个θ，使的θ.T*x >= 1。

类似的，对于训练样本标签y=0，需找到一个θ，是的θ.T*x <= -1。

当第一项为0时，可以得到决策边界，如下图：

![ML-seventh14](https://github.com/yiyading/NLP-and-ML/blob/master/img_ML2/ML-seventh14.png)

具体而言，如果有这样一个数据集，其中有正样本和负样本，可以清晰的看出来这个数据集是可分的。即存在直线将正负样本分开。

如下如所示，黄色和粉色的线分别代表一种决策边界，但我们可以看出黑色的线所代表的决策边界有更大的最短距离，这两条额外的蓝色线就表示黑色线决策边界与训练样本的最短距离。

![ML-seventh15](https://github.com/yiyading/NLP-and-ML/blob/master/img_ML2/ML-seventh15.png)

> 这个间距叫做支持向量机的间距，而这正是支持向量机有鲁棒性的原因。

上图努力用一个最大间距分离样本，因此支持向量机有时被称为最大间距分类器。

**对SVM进一步理解**

在代价函数最小化的过程中，我们希望找出在y=1和y=0两种情况下都使得代价函数中左边这一项尽量为0的参数，因此我们的最小化问题就发生了转变，如下图：

![ML-seventh16](https://github.com/yiyading/NLP-and-ML/blob/master/img_ML2/ML-seventh16.png)

> 熟悉这个限制条件，这个限制条件在下一节的数学推到中非常重要。

事实上，支持向量机现在要比这个大间距分类器所体现的更成熟，尤其在使用大间距分类器时，学习算法会受到异常点的影响。比如加入一个额外的正样本，如下图所示：

![ML-seventh17](https://github.com/yiyading/NLP-and-ML/blob/master/img_ML2/ML-seventh17.png)

因为加入了一个额外的样本，为了将样本用最大间距区分开，我们会得到类似粉色线的决策边界。仅仅是这样一个样本，就将决策边界从黑色的线变成粉色的线是不明智的。

在SVM中，如果C设置的非常大，且出现了线性可分的数据，就会出现黑线->粉线的情况，**如果C设置的不是太大，最终会得到黑线**。

但如果出现非线性可分的样本，即在正样本中有负样本或者负样本中有正样本，则SVM还是能将他们恰当的分开，即黑线。

C的作用类似于1/λ，λ是我们之前使用过的正则化参数，这只是C非常大的情形，或者等价λ非常小的情形，最终会得到粉线（决策分界线）。**但如果C不是非常大的时候，它可以忽略到一些异常点的影响，得到更好的决策边界**。

> 回顾C=1/λ，因此：<br>
> C较大时，相当于λ较小，可能会导致过拟合，高方差<br>
> C较小时，相当于λ较大，可能会导致欠拟合，高偏差<br>

直观理解，SVM就是用最大间距将样本区分开来。

## 3.数学背后的大边界分类

目标函数希望找到一个参数θ，它的范数是最小的。

在上一节我们提到如果想要的到SVM的决策边界，要符合下图所示的条件：

![ML-seventh18](https://github.com/yiyading/NLP-and-ML/blob/master/img_ML2/ML-seventh18.png)

为了简化计算，我们假设只有2个特征，并且θ0=0(忽略截距)。||u||是y的范数，表示u 的长度。

![ML-seventh19](https://github.com/yiyading/NLP-and-ML/blob/master/img_ML2/ML-seventh19.png)

> 内积知识如果不太会，直接百度即可，这是线代里面比较基础的内容。
如上图，当满足限制条件时，前项为零，只求最后一项θ平方的叠加和的最小值即可。当只有两个特征时，其平方和等于θ向量的范数||θ||^2。因此SVM做的全部事情就是**极小化参数向量θ范数的平方，或者说是长度的平方**。

做一下内积转换，下图中||θ||是参数θ的长度，p是x在θ向量的投影（上图有所展示）：

![ML-seventh20](https://github.com/yiyading/NLP-and-ML/blob/master/img_ML2/ML-seventh20.png)
> 在这张图中可以清晰的看到，我们的优化目标函数可以被写成=(1/2)\*||θ||^2。

## 4.SVM如何选择决策边界
在以上的内容我们对目标函数进行内积变化，求其最优解转化成了求θ范数||θ||平方的最小值。

θ0=0仅仅意味着决策边界必须通过远点(0,0)。

![ML-seventh21](https://github.com/yiyading/NLP-and-ML/blob/master/img_ML2/ML-seventh21.png)

上图中绿色的线表示一条决策边界，此时参数向量θ与其正交。
> 为什么参数向量θ和决策边界要正交？<br>
> 答：为了保证决策边界成为正负分割线，要使得不同样本x在θ向量上的投影p\*||θ||大于1或者小于-1.

对于图中的两个样本x1和x2（图中1和2用小括号上标），要满足决策边界的限制条件
> p\*||θ|| >= 1<br>
> p\*||θ|| <= -1<br>

从图中可以观察到两个样本在x在θ向量上的投影p都非常小，如果要满足限制条件，就必须要保证θ的范数||θ||非常大，达不到极小化||θ||平方和的要求。

相反的，如果SVM选择了下边这个决策边界

![ML-seventh22](https://github.com/yiyading/NLP-and-ML/blob/master/img_ML2/ML-seventh22.png)

在这种条件下，样本x在θ向量方向的投影p变大，在满足限制条件的前提下，||θ||就会变小。这条决策边界极小化||θ||平方和的效果比上边那条决策边界要好。

* 我们希望正负样本投影到θ的值p很大，要做到这一点，唯一的方式就是选择这条绿色的线作为决策边界。这个间距就是p1，p2等等的值。通过让间距变大，SVM最终可以找到一个较小的θ范数。

## 5.核函数
当我们解决无法用直线进行分割的分类问题时，我们需要使用次数更高的函数来分割模型。

![ML-seventh23](https://github.com/yiyading/NLP-and-ML/blob/master/img_ML2/ML-seventh23.png)

在上图的描述中，我们对高阶模型中的特征进行了变换：
> hθ(x) = 1 , if θ0+θ1x1+θ2x2+... >= 0<br>
> hθ(x) = 0 , otherwise<br>
> <br>
> 在进行模型变换后：
> hθ(x) = 1 , if θ0+θ1f1+θ2f2+... >= 0<br>
> hθ(x) = 0 , otherwise<br>

除了对原有特征进行组合外，我们还可以利用核函数计算出新的特征f1,f2,f3...

![ML-seventh24](https://github.com/yiyading/NLP-and-ML/blob/master/img_ML2/ML-seventh24.png)

这里面用到了**高斯核函数**，虽然看上去和正态分布很像，但实际上和正态分布没什么实际关系。

高斯核函数的假设如下：

![ML-seventh25](https://github.com/yiyading/NLP-and-ML/blob/master/img_ML2/ML-seventh25.png)

其中范数为实例x与地标(landmarks)之间的距离的平方：

![ML-seventh26](https://github.com/yiyading/NLP-and-ML/blob/master/img_ML2/ML-seventh26.png)
> 如果一个训练实例x与地标l之间的距离近似于0，则新特征f=exp(-0)约等于1。<br>
> 如果一个训练实例x与地标l之间的举例较远，则新特征f=exp(||large num||^2)约等于0。

我们假设训练实例含有两个特征[x1, x2]，给定地标l(1)与不同的σ值，见下图：

![ML-seventh27](https://github.com/yiyading/NLP-and-ML/blob/master/img_ML2/ML-seventh27.png)

图中水平坐标为x1,x2，垂直坐标代表f。可以看出，只有当x与l(1)重合时，f才是最大值1。随着x的改变，f值改变的速率受到σ值的控制。

![ML-seventh28](https://github.com/yiyading/NLP-and-ML/blob/master/img_ML2/ML-seventh28.png)

在上图中红线内部预测y=1，红线外预测y=0。

当满足hθ()=θ0+θ1\*f1+θ2\*f2+θ3\*f3 >= 0时，y=1；否则y=0。

例如，对于绿色的点，比较接近l(2)，因此f2约等于1，f1/f3都等于0，满足hθ()>0，其预测y=1。

## 6.核函数2
如何选择地标（landmarks）的数量？

根据训练集数量来选，如果有m个实例，则我们选取m个地标，并令x(i)=l(i)（i表示第几个实例和地标）<br>

![ML-seventh29](https://github.com/yiyading/NLP-and-ML/blob/master/img_ML2/ML-seventh29.png)

如下图，在计算θ\*f时，需要用x与所有landmarks进行计算，计算出f(i)，然后累加。

![ML-seventh30](https://github.com/yiyading/NLP-and-ML/blob/master/img_ML2/ML-seventh30.png)

将高斯核函数运用到SVM中，修改SVM的假设：

* 给定x，计算新特征f，当θ.T\*f >= 0时，预测y=1，否则反之。相应的修改代价函数为：

![ML-seventh31](https://github.com/yiyading/NLP-and-ML/blob/master/img_ML2/ML-seventh31.png)
> 注意上面单价函数与SVM中不同点在于cost函数内部的参数发生了变化，即特征的选择发生了变化。

上式中的正则化项可以进行如下的转换，目的时为了适应不同的核函数。

![ML-seventh32](https://github.com/yiyading/NLP-and-ML/blob/master/img_ML2/ML-seventh32.png)

在上边这个转化中，我们用θ.T\*M\*θ来代替θ.T\*θ，这里的M是根据选择的核函数不同而不同的一个矩阵，这么做的原因是为了简化计算。

这里我们不介绍如何进行最小化SVM的代价函数，我们可以使用libsvm软件包。

在使用软件包最小化代价函数前，通常需要编写合函数，并且如果我们使用高斯核函数，那么在使用之前进行特征缩放是非常有必要的。

下面时支持向量机的两个参数C核σ的影响：

C=1/λ 
1. C较大时，相当于λ较小，可能会导致过拟合，高方差。
2. C较小时，相当于λ较大，可能会导致欠拟合，高偏差。

σ
1. σ较大时，可能会导致低方差，高偏差。
2. σ较小时，可能会导致低偏差，高方差。

## 7.使用SVM
在运用SVM时，非常不建议自己写一个软件来求解参数θ，当经已经有很多高优化的软件库能够实现这些功能，比如说liblinear核libsvm。

在高斯合函数之外我们还有一些其他选择，如：
1. 多项式核函数
2. 字符串核函数
3. 卡方核函数
4. 直方图交集核函数

这些核函数的目标也都是根据训练集与地标之间的距离来构建新特征。

尽管不需要自己写SVM来优化软件，但还是需要做几件事情：
1. 对参数C进行选择。
2. 选择内核参数核想要使用的相似函数，其中一个选择是：不需要任何内核参数，也叫线性核函数。因此如果有人说他使用了**线性核的SVM，就意味着他使用了不带核函数的SVM**。

**一些普遍的使用规则**

n表示特征数，m表示训练样本数。

1. 如果m远远小于n，数据量不支持我们训练一个复杂的非线性模型，我们选用逻辑回归模型或不带核函数的SVM。

2. 如果n较小，且m大小中等，例如n在1-1000之间，m在10-10000之间，使用高斯核函数的SVM。

3. 如果n远小于m，使用SVM会非常慢，解决方案时创造、增加更多的特征，然后使用逻辑回归或不带核函数的SVM。

key import：选用NN或者SVM或者带核的SVM，这些算法的不同点在于不同的情况下训练速度会非常不同。
